---
title: "Homework 9 - P8105 Probability"
author: "Yongyan Liu (yl6107)"
date: "Due: November 19, 2025"
output: pdf_document
geometry: margin=1in
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
---

# Problem 1

Cast a fair die and let $X = 0$ if 1, 2, or 3 spots appear, let $X = 1$ if 4 or 5 spots appear, and let $X = 2$ if 6 spots appear. Do this two independent times, obtaining $X_1$ and $X_2$. Calculate $P(|X_1 - X_2| = 1)$.

**Solution:**

First, we determine the probability distribution of $X$:

- $P(X = 0) = P(\text{1, 2, or 3}) = \frac{3}{6} = \frac{1}{2}$
- $P(X = 1) = P(\text{4 or 5}) = \frac{2}{6} = \frac{1}{3}$
- $P(X = 2) = P(\text{6}) = \frac{1}{6}$

Since $X_1$ and $X_2$ are independent, we have the following joint probabilities:

| $X_1 \backslash X_2$ | 0 | 1 | 2 |
|:---:|:---:|:---:|:---:|
| 0 | $\frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}$ | $\frac{1}{2} \cdot \frac{1}{3} = \frac{1}{6}$ | $\frac{1}{2} \cdot \frac{1}{6} = \frac{1}{12}$ |
| 1 | $\frac{1}{3} \cdot \frac{1}{2} = \frac{1}{6}$ | $\frac{1}{3} \cdot \frac{1}{3} = \frac{1}{9}$ | $\frac{1}{3} \cdot \frac{1}{6} = \frac{1}{18}$ |
| 2 | $\frac{1}{6} \cdot \frac{1}{2} = \frac{1}{12}$ | $\frac{1}{6} \cdot \frac{1}{3} = \frac{1}{18}$ | $\frac{1}{6} \cdot \frac{1}{6} = \frac{1}{36}$ |

The event $|X_1 - X_2| = 1$ occurs when:
- $(X_1, X_2) = (0, 1)$: $|0 - 1| = 1$
- $(X_1, X_2) = (1, 0)$: $|1 - 0| = 1$
- $(X_1, X_2) = (1, 2)$: $|1 - 2| = 1$
- $(X_1, X_2) = (2, 1)$: $|2 - 1| = 1$

Therefore:
$$P(|X_1 - X_2| = 1) = P(X_1 = 0, X_2 = 1) + P(X_1 = 1, X_2 = 0) + P(X_1 = 1, X_2 = 2) + P(X_1 = 2, X_2 = 1)$$

$$= \frac{1}{6} + \frac{1}{6} + \frac{1}{18} + \frac{1}{18} = \frac{4}{9}$$

# Problem 2

Let $X$ and $Y$ be independent standard normal random variables.

## Part (a)

Show that $\frac{X}{X+Y}$ has a Cauchy distribution.

**Solution:**

We use the bivariate transformation method. Let $U = \frac{X}{X+Y}$ and $V = X + Y$. We will find the joint pdf of $(U, V)$ and then find the marginal pdf of $U$.

From the definitions:
$$U = \frac{X}{X+Y}, \quad V = X + Y$$

We can solve for $X$ and $Y$:
$$X = UV, \quad Y = V - X = V - UV = V(1-U)$$
$$J = \begin{vmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{vmatrix} = \begin{vmatrix}
v & u \\
-v & 1-u
\end{vmatrix} = v(1-u) + uv = v$$

Since $X, Y \sim N(0,1)$ are independent:
$$f_{X,Y}(x,y) = \frac{1}{2\pi} e^{-\frac{1}{2}(x^2 + y^2)}$$

The joint pdf of $(U, V)$ is:
$$f_{U,V}(u,v) = f_{X,Y}(uv, v(1-u)) \cdot |J|$$

$$= \frac{1}{2\pi} \exp\left\{-\frac{1}{2}\left[(uv)^2 + (v(1-u))^2\right]\right\} \cdot |v|$$

$$= \frac{|v|}{2\pi} \exp\left\{-\frac{v^2}{2}[u^2 + (1-u)^2]\right\}$$

$$= \frac{|v|}{2\pi} \exp\left\{-\frac{v^2}{2}[u^2 + 1 - 2u + u^2]\right\}$$

$$= \frac{|v|}{2\pi} \exp\left\{-\frac{v^2}{2}[2u^2 - 2u + 1]\right\}$$

for $-\infty < u < \infty$ and $-\infty < v < \infty$.

$$f_U(u) = \int_{-\infty}^{\infty} f_{U,V}(u,v) \, dv = \int_{-\infty}^{\infty} \frac{|v|}{2\pi} \exp\left\{-\frac{v^2}{2}[2u^2 - 2u + 1]\right\} dv$$

Since the integrand is an even function of $v$:
$$f_U(u) = \frac{2}{2\pi} \int_{0}^{\infty} v \exp\left\{-\frac{v^2}{2}[2u^2 - 2u + 1]\right\} dv$$

Let $w = \frac{v^2}{2}(2u^2 - 2u + 1)$. Then $dw = v(2u^2 - 2u + 1) \, dv$, so $v \, dv = \frac{dw}{2u^2 - 2u + 1}$.

$$f_U(u) = \frac{1}{\pi} \int_{0}^{\infty} \frac{e^{-w}}{2u^2 - 2u + 1} dw = \frac{1}{\pi(2u^2 - 2u + 1)} \left[e^{-w}\right]_0^{\infty} = \frac{1}{\pi(2u^2 - 2u + 1)}$$

Completing the square in the denominator:
$$2u^2 - 2u + 1 = 2\left(u^2 - u + \frac{1}{2}\right) = 2\left(u - \frac{1}{2}\right)^2 + \frac{1}{2}$$

This can be rewritten as:
$$f_U(u) = \frac{1}{\pi \cdot \frac{1}{2} \left[1 + \left(\frac{u - 1/2}{1/2}\right)^2\right]}$$

So, this is a **Cauchy distribution** with location parameter $x_0 = \frac{1}{2}$ and scale parameter $\gamma = \frac{1}{2}$.

## Part (b)

Find the pdf of $\frac{X}{|Y|}$ and identify what distribution it is.

**Hint:** Find the joint pdf of $U = \frac{X}{|Y|}$ and $V = |Y|$ first.

**Solution:**

Following the hint, let $U = \frac{X}{|Y|}$ and $V = |Y|$.

Since $V = |Y|$, we need to consider two cases:

- **Case 1:** When $Y > 0$, we have $V = Y$, so:
  $$X = UV, \quad Y = V$$

- **Case 2:** When $Y < 0$, we have $V = -Y$, so:
  $$X = UV, \quad Y = -V$$

For Case 1 ($Y > 0$):
$$J_1 = \begin{vmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{vmatrix} = \begin{vmatrix}
v & u \\
0 & 1
\end{vmatrix} = v$$

For Case 2 ($Y < 0$):
$$J_2 = \begin{vmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{vmatrix} = \begin{vmatrix}
v & u \\
0 & -1
\end{vmatrix} = -v$$

In both cases, $|J| = v$ (noting that $v > 0$ since $V = |Y| \geq 0$).

Since $X$ and $Y$ are independent standard normal variables:
$$f_{X,Y}(x,y) = \frac{1}{2\pi} e^{-\frac{1}{2}(x^2 + y^2)}$$

By the transformation formula, accounting for both cases:

$$f_{U,V}(u,v) = f_{X,Y}(uv, v) \cdot |J_1| + f_{X,Y}(uv, -v) \cdot |J_2|$$

$$= \frac{1}{2\pi} e^{-\frac{1}{2}(u^2v^2 + v^2)} \cdot v + \frac{1}{2\pi} e^{-\frac{1}{2}(u^2v^2 + v^2)} \cdot v$$

$$= \frac{2v}{2\pi} e^{-\frac{v^2}{2}(u^2 + 1)} = \frac{v}{\pi} e^{-\frac{v^2}{2}(u^2 + 1)}$$

for $-\infty < u < \infty$ and $v > 0$.

$$f_U(u) = \int_0^{\infty} f_{U,V}(u,v) \, dv = \int_0^{\infty} \frac{v}{\pi} e^{-\frac{v^2}{2}(u^2 + 1)} dv$$

Let $w = \frac{v^2}{2}(u^2 + 1)$. Then $dw = v(u^2 + 1) \, dv$, so $v \, dv = \frac{dw}{u^2 + 1}$.

$$f_U(u) = \frac{1}{\pi} \int_0^{\infty} \frac{e^{-w}}{u^2 + 1} dw = \frac{1}{\pi(u^2 + 1)} \int_0^{\infty} e^{-w} dw = \frac{1}{\pi(1 + u^2)}$$

for $-\infty < u < \infty$. This is the **standard Cauchy distribution**.

# Problem 3

$X_1$ and $X_2$ are independent random variables following $N(0, \sigma^2)$.

## Part (a)

Find the joint distribution of $Y_1 = X_1^2 + X_2^2$ and $Y_2 = \frac{X_1}{\sqrt{Y_1}}$.

**Solution:**

We first find the marginal distribution of $Y_1$, then use an auxiliary variable to find the distribution of $Y_2$, and finally establish their joint distribution.

Since $X_1, X_2 \sim N(0, \sigma^2)$ independently, we can standardize: $Z_i = \frac{X_i}{\sigma} \sim N(0,1)$.

Then:
$$Y_1 = X_1^2 + X_2^2 = \sigma^2(Z_1^2 + Z_2^2)$$

We know that $Z_i^2 \sim \chi^2(1)$ for each $i$, and since $Z_1$ and $Z_2$ are independent, $Z_1^2 + Z_2^2 \sim \chi^2(2)$.

Let $W = Z_1^2 + Z_2^2 \sim \chi^2(2)$ with pdf $f_W(w) = \frac{1}{2}e^{-w/2}$ for $w > 0$.

Since $Y_1 = \sigma^2 W$, using the transformation $y_1 = \sigma^2 w$, we have $\frac{dy_1}{dw} = \sigma^2$, so:
$$f_{Y_1}(y_1) = f_W\left(\frac{y_1}{\sigma^2}\right) \cdot \frac{1}{\sigma^2} = \frac{1}{2}e^{-y_1/(2\sigma^2)} \cdot \frac{1}{\sigma^2} = \frac{1}{2\sigma^2}e^{-y_1/(2\sigma^2)}$$

for $y_1 > 0$.

For bivariate normal $(X_1, X_2)$ with independent, identically distributed components, the direction is uniformly distributed. Let $\Theta$ denote the angle, where $\Theta \sim \text{Uniform}(0, 2\pi)$ with pdf $f_{\Theta}(\theta) = \frac{1}{2\pi}$.

Then $Y_2 = \cos(\Theta)$. For $-1 < y_2 < 1$, the equation $y_2 = \cos(\theta)$ has two solutions in $[0, 2\pi)$: $\theta_1 = \arccos(y_2)$ and $\theta_2 = 2\pi - \arccos(y_2)$.

Using the transformation formula with $\frac{d}{dy_2}\arccos(y_2) = -\frac{1}{\sqrt{1-y_2^2}}$:
$$f_{Y_2}(y_2) = f_{\Theta}(\theta_1)\left|\frac{d\theta_1}{dy_2}\right| + f_{\Theta}(\theta_2)\left|\frac{d\theta_2}{dy_2}\right| = \frac{1}{2\pi} \cdot \frac{1}{\sqrt{1-y_2^2}} + \frac{1}{2\pi} \cdot \frac{1}{\sqrt{1-y_2^2}} = \frac{1}{\pi\sqrt{1-y_2^2}}$$

for $-1 < y_2 < 1$.

For bivariate normal with independent, identically distributed components, the squared radius $Y_1 = X_1^2 + X_2^2$ is independent of the directional component $Y_2 = \frac{X_1}{\sqrt{X_1^2 + X_2^2}}$ due to the rotational symmetry of the distribution.

Therefore:
$$f_{Y_1, Y_2}(y_1, y_2) = f_{Y_1}(y_1) \cdot f_{Y_2}(y_2) = \frac{1}{2\sigma^2} e^{-y_1/(2\sigma^2)} \cdot \frac{1}{\pi\sqrt{1-y_2^2}}$$

for $y_1 > 0$ and $-1 < y_2 < 1$.

## Part (b)

Show that $Y_1$ and $Y_2$ are independent, and interpret this result geometrically.

**Solution:**

From part (a), we already established that $Y_1$ and $Y_2$ are independent due to the rotational symmetry of the bivariate normal distribution. 

For bivariate normal random variables $(X_1, X_2)$ with independent components and equal variances:

- $Y_1 = X_1^2 + X_2^2$ represents the squared distance from the origin
- $Y_2 = \frac{X_1}{\sqrt{X_1^2 + X_2^2}}$ represents a normalized directional component

This independence reflects the **rotational symmetry** of the bivariate normal distribution with independent, identically distributed components. The distance from the origin (measured by $Y_1$) is independent of the direction (captured by $Y_2$), which is a fundamental property of spherically symmetric distributions.

# Problem 4

A point is generated at random in the plane according to the following polar scheme: A radius $R$ is chosen, where $R^2 \sim \chi^2(2)$. Independently, an angle $\theta$ is chosen, where $\theta \sim \text{Uniform}(0, 2\pi)$. Find the joint distribution of $X = R\cos\theta$ and $Y = R\sin\theta$.

**Solution:**

We recognize that this problem is the reverse of Problem 3.

Since that $X^2 + Y^2 = R^2\cos^2\Theta + R^2\sin^2\Theta = R^2 \sim \chi^2(2)$, the setup here matches exactly the conditions from Problem 3 (with $\sigma^2 = 1$), and the relationship between bivariate normal and the chi-squared/uniform representation is one-to-one, we can conclude that $(X, Y)$ must be independent standard normal random variables.

The joint pdf is:
$$f_{X,Y}(x,y) = \frac{1}{2\pi} e^{-(x^2+y^2)/2} = \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \cdot \frac{1}{\sqrt{2\pi}} e^{-y^2/2} = \frac{1}{2\pi} e^{-(x^2+y^2)/2}, \quad -\infty < x, y < \infty$$

# Problem 5

Let $(X_1, X_2, X_3)$ have joint pdf
$$f_{X_1,X_2,X_3}(x_1, x_2, x_3) = \begin{cases}
48 x_1 x_2 x_3, & 0 < x_1 < x_2 < x_3 < 1 \\
0, & \text{otherwise}
\end{cases}$$

Define the transformation $Y_1 = \frac{X_1}{X_2}$, $Y_2 = \frac{X_2}{X_3}$, $Y_3 = X_3$.

## Part (a)

Find the joint pdf of $Y_1, Y_2$, and $Y_3$.

**Solution:**

We use the multivariate transformation method. Solving for $X_1, X_2, X_3$:

- From $Y_3 = X_3$: $X_3 = Y_3$
- From $Y_2 = \frac{X_2}{X_3}$: $X_2 = Y_2 X_3 = Y_2 Y_3$
- From $Y_1 = \frac{X_1}{X_2}$: $X_1 = Y_1 X_2 = Y_1 Y_2 Y_3$

$$J = \begin{vmatrix}
\frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} & \frac{\partial x_1}{\partial y_3} \\
\frac{\partial x_2}{\partial y_1} & \frac{\partial x_2}{\partial y_2} & \frac{\partial x_2}{\partial y_3} \\
\frac{\partial x_3}{\partial y_1} & \frac{\partial x_3}{\partial y_2} & \frac{\partial x_3}{\partial y_3}
\end{vmatrix} = \begin{vmatrix}
y_2 y_3 & y_1 y_3 & y_1 y_2 \\
0 & y_3 & y_2 \\
0 & 0 & 1
\end{vmatrix}$$

This is an upper triangular matrix, so the determinant is the product of the diagonal elements:
$$J = (y_2 y_3) \cdot y_3 \cdot 1 = y_2 y_3^2$$

From the original constraint $0 < x_1 < x_2 < x_3 < 1$, the support is: $0 < y_1 < 1$, $0 < y_2 < 1$, $0 < y_3 < 1$.

Using the transformation formula:
$$
\begin{aligned}
f_{Y_1,Y_2,Y_3}(y_1, y_2, y_3) 
&=f_{X_1,X_2,X_3}(x_1(y_1, y_2, y_3), x_2(y_1, y_2, y_3), x_3(y_1, y_2, y_3)) \cdot |J| \\
&=48(y_1 y_2 y_3)(y_2 y_3)(y_3) \cdot y_2 y_3^2 \\
&=48 y_1 y_2^3 y_3^5
\end{aligned}
$$
for $0 < y_1, y_2, y_3 < 1$.

**Answer:**
$$
f_{Y_1,Y_2,Y_3}(y_1, y_2, y_3) = \begin{cases}
48 y_1 y_2^3 y_3^5, & 0 < y_1, y_2, y_3 < 1 \\
0, & \text{otherwise}
\end{cases}
$$

## Part (b)

Are $Y_1, Y_2$, and $Y_3$ mutually independent?

**Solution:**

We can rewrite:
$$f_{Y_1,Y_2,Y_3}(y_1, y_2, y_3) = 48 y_1 y_2^3 y_3^5 = (2y_1) \cdot (4y_2^3) \cdot (6y_3^5)$$

for $0 < y_1, y_2, y_3 < 1$.

Let's verify these are valid pdfs:

- $\int_0^1 2y_1 \, dy_1 = 2 \cdot \frac{y_1^2}{2}\Big|_0^1 = 1$
- $\int_0^1 4y_2^3 \, dy_2 = 4 \cdot \frac{y_2^4}{4}\Big|_0^1 = 1$
- $\int_0^1 6y_3^5 \, dy_3 = 6 \cdot \frac{y_3^6}{6}\Big|_0^1 = 1$

Since
$$f_{Y_1,Y_2,Y_3}(y_1, y_2, y_3) = (2y_1) \cdot (4y_2^3) \cdot (6y_3^5) = f_{Y_1}(y_1) \cdot f_{Y_2}(y_2) \cdot f_{Y_3}(y_3)$$

where:

- $f_{Y_1}(y_1) = 2y_1$ for $0 < y_1 < 1$
- $f_{Y_2}(y_2) = 4y_2^3$ for $0 < y_2 < 1$
- $f_{Y_3}(y_3) = 6y_3^5$ for $0 < y_3 < 1$

Yes, $Y_1, Y_2$, and $Y_3$ are **mutually independent**.
